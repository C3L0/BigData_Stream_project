{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "316274d5-6cdd-4ee5-9894-871b52fdcdd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Big Data Project<br>\n",
    "####Made by:<br>\n",
    "* Paul ROUXEL\n",
    "* Victor JOUET\n",
    "* Antoine LOPEZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0321b38-8b6e-4c4c-bf4c-c7407d2db39f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pywikibot in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7faf7bff-513e-482e-9f25-fa0ce2ce2e62/lib/python3.12/site-packages (10.7.4)\nRequirement already satisfied: mwparserfromhell>=0.5.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7faf7bff-513e-482e-9f25-fa0ce2ce2e62/lib/python3.12/site-packages (from pywikibot) (0.7.2)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from pywikibot) (24.1)\nRequirement already satisfied: requests>=2.31.0 in /databricks/python3/lib/python3.12/site-packages (from pywikibot) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.31.0->pywikibot) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.31.0->pywikibot) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.31.0->pywikibot) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.31.0->pywikibot) (2025.1.31)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: requests-sse in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7faf7bff-513e-482e-9f25-fa0ce2ce2e62/lib/python3.12/site-packages (0.5.2)\nRequirement already satisfied: requests<3.0.0,>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from requests-sse) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->requests-sse) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->requests-sse) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->requests-sse) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->requests-sse) (2025.1.31)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install pywikibot\n",
    "%pip install requests-sse\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb912be-cbe4-4f7b-8df2-92a245748682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb012ff8-d385-470b-8a51-e330a869fcbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    BooleanType, LongType, MapType\n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import json\n",
    "from pywikibot.comms.eventstreams import EventStreams\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    LongType,\n",
    "    BooleanType\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c912211-eba2-4e44-bfd2-7ef15cf7efb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb76536-0df6-45f9-b7d4-c1889174f37a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data source: https://datasets.imdbws.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4573b712-be78-41fc-894b-f33e10795623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Databricks cluster configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "136611bb-9d72-445f-be9b-ff36ca6889e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = 'imdb_db'\n",
    "uc_schema_raw_events = 'raw_events'\n",
    "raw_data_volume = 'imdb_raw' \n",
    "raw_data_path = f'/Volumes/{catalog}/{uc_schema_raw_events}/{raw_data_volume}'\n",
    "db_schema_checkpoints = 'checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d13e2076-fe10-4723-8a2a-a11015b2284d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Catalog creation\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\") \n",
    "\n",
    "# Schemas creation\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{uc_schema_raw_events}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db_schema_checkpoints}\")\n",
    "\n",
    "# Volume creation\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{uc_schema_raw_events}.{raw_data_volume}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de3e0e22-42aa-4b78-aa6e-18dd6958c3ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# IMDB files\n",
    "BASE_URL = \"https://datasets.imdbws.com/\"\n",
    "FILE_NAMES: List[str] = [\n",
    "    \"name.basics.tsv.gz\",\n",
    "    \"title.akas.tsv.gz\",\n",
    "    \"title.basics.tsv.gz\",\n",
    "    \"title.crew.tsv.gz\",\n",
    "    \"title.episode.tsv.gz\",\n",
    "    \"title.principals.tsv.gz\",\n",
    "    \"title.ratings.tsv.gz\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5670d56e-f8ce-4207-a033-d7bdc4123a4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Volume\n",
    "dbutils.fs.mkdirs(raw_data_path)\n",
    "\n",
    "DBFS_PATH = raw_data_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abcde112-9b8c-465a-840d-7d821b1a1bf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Download from extern URL to the DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa064cda-b8db-4c28-8ecc-2578d211bad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DBFS_PATH = /Volumes/imdb_db/raw_events/imdb_raw\n",
    "LOCAL_TMP_PATH = \"/tmp/imdb_downloads\"\n",
    "os.makedirs(LOCAL_TMP_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d0994e-1559-4e4a-a3b6-41070ea1a890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def download_and_copy(file_name: str) -> str | None:\n",
    "    \"\"\"Télécharge un fichier et le copie vers le Volume UC.\"\"\"\n",
    "    source_url = BASE_URL + file_name\n",
    "    local_file_path = os.path.join(LOCAL_TMP_PATH, file_name)\n",
    "    dbfs_file_path = DBFS_PATH + \"/\" + file_name \n",
    "    \n",
    "    try:\n",
    "        # Download file to local temporaty file system\n",
    "        with requests.get(source_url, stream=True, timeout=300) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(local_file_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192 * 1024):\n",
    "                    f.write(chunk)\n",
    "        \n",
    "        # Copy file to DBFS\n",
    "        dbutils.fs.cp(f\"file:{local_file_path}\", dbfs_file_path, recurse=True)\n",
    "        print(f\"Succès: {file_name} copié vers {dbfs_file_path}\")\n",
    "\n",
    "        os.remove(local_file_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error while loading {file_name}: {e}\")\n",
    "        print(f\"URL: {source_url}\")\n",
    "        return file_name\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be106020-5f64-41b7-949f-068f683210ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while loading title.episode.tsv.gz: (java.lang.SecurityException) Cannot use com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem - local filesystem access is forbidden\n\nJVM stacktrace:\njava.lang.SecurityException\n\tat com.databricks.hadoop.safety.FileSystemCallSiteAllowlist.assertCallSiteIsAllowed(FileSystemCallSiteAllowlist.java:51)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$2(DBUtilsCore.scala:220)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:210)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$1(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:210)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withCpSafetyChecks(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$2(DBUtilsCore.scala:636)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:204)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$1(DBUtilsCore.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:104)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:104)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$recordDbutilsFsOp$6(DBUtilsCore.scala:186)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:186)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.cp(DBUtilsCore.scala:635)\n\tat com.databricks.sql.DbutilsFsSparkConnectBridgeImpl.cp(DbutilsFsSparkConnectBridgeImpl.scala:66)\n\tat com.databricks.sql.managedcatalog.command.CopyDbutilsCommand.run(DbutilsFsCommands.scala:120)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1605)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:701)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:693)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:693)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:693)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:431)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$11(Dataset.scala:244)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:238)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:113)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:376)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)\nURL: https://datasets.imdbws.com/title.episode.tsv.gz\nError while loading title.crew.tsv.gz: (java.lang.SecurityException) Cannot use com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem - local filesystem access is forbidden\n\nJVM stacktrace:\njava.lang.SecurityException\n\tat com.databricks.hadoop.safety.FileSystemCallSiteAllowlist.assertCallSiteIsAllowed(FileSystemCallSiteAllowlist.java:51)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$2(DBUtilsCore.scala:220)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:210)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$1(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:210)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withCpSafetyChecks(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$2(DBUtilsCore.scala:636)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:204)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$1(DBUtilsCore.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:104)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:104)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$recordDbutilsFsOp$6(DBUtilsCore.scala:186)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:186)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.cp(DBUtilsCore.scala:635)\n\tat com.databricks.sql.DbutilsFsSparkConnectBridgeImpl.cp(DbutilsFsSparkConnectBridgeImpl.scala:66)\n\tat com.databricks.sql.managedcatalog.command.CopyDbutilsCommand.run(DbutilsFsCommands.scala:120)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1605)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:701)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:693)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:693)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:693)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:431)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$11(Dataset.scala:244)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:238)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:113)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:376)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)\nURL: https://datasets.imdbws.com/title.crew.tsv.gz\nError while loading title.basics.tsv.gz: (java.lang.SecurityException) Cannot use com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem - local filesystem access is forbidden\n\nJVM stacktrace:\njava.lang.SecurityException\n\tat com.databricks.hadoop.safety.FileSystemCallSiteAllowlist.assertCallSiteIsAllowed(FileSystemCallSiteAllowlist.java:51)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$2(DBUtilsCore.scala:220)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:210)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$1(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:210)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withCpSafetyChecks(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$2(DBUtilsCore.scala:636)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:204)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$1(DBUtilsCore.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:104)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:104)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$recordDbutilsFsOp$6(DBUtilsCore.scala:186)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:186)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.cp(DBUtilsCore.scala:635)\n\tat com.databricks.sql.DbutilsFsSparkConnectBridgeImpl.cp(DbutilsFsSparkConnectBridgeImpl.scala:66)\n\tat com.databricks.sql.managedcatalog.command.CopyDbutilsCommand.run(DbutilsFsCommands.scala:120)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1605)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:701)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:693)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:693)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:693)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:431)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$11(Dataset.scala:244)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:238)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:113)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:376)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)\nURL: https://datasets.imdbws.com/title.basics.tsv.gz\nError while loading title.ratings.tsv.gz: (java.lang.SecurityException) Cannot use com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem - local filesystem access is forbidden\n\nJVM stacktrace:\njava.lang.SecurityException\n\tat com.databricks.hadoop.safety.FileSystemCallSiteAllowlist.assertCallSiteIsAllowed(FileSystemCallSiteAllowlist.java:51)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$2(DBUtilsCore.scala:220)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:210)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$1(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:210)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withCpSafetyChecks(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$2(DBUtilsCore.scala:636)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:204)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$1(DBUtilsCore.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:104)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:104)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$recordDbutilsFsOp$6(DBUtilsCore.scala:186)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:186)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.cp(DBUtilsCore.scala:635)\n\tat com.databricks.sql.DbutilsFsSparkConnectBridgeImpl.cp(DbutilsFsSparkConnectBridgeImpl.scala:66)\n\tat com.databricks.sql.managedcatalog.command.CopyDbutilsCommand.run(DbutilsFsCommands.scala:120)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1605)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:701)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:693)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:693)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:693)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:431)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$11(Dataset.scala:244)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:238)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:113)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:376)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)\nURL: https://datasets.imdbws.com/title.ratings.tsv.gz\nError while loading name.basics.tsv.gz: (java.lang.SecurityException) Cannot use com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem - local filesystem access is forbidden\n\nJVM stacktrace:\njava.lang.SecurityException\n\tat com.databricks.hadoop.safety.FileSystemCallSiteAllowlist.assertCallSiteIsAllowed(FileSystemCallSiteAllowlist.java:51)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$2(DBUtilsCore.scala:220)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:210)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$1(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:210)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withCpSafetyChecks(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$2(DBUtilsCore.scala:636)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:204)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$1(DBUtilsCore.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:104)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:104)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$recordDbutilsFsOp$6(DBUtilsCore.scala:186)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:186)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.cp(DBUtilsCore.scala:635)\n\tat com.databricks.sql.DbutilsFsSparkConnectBridgeImpl.cp(DbutilsFsSparkConnectBridgeImpl.scala:66)\n\tat com.databricks.sql.managedcatalog.command.CopyDbutilsCommand.run(DbutilsFsCommands.scala:120)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1605)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:701)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:693)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:693)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:693)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:431)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$11(Dataset.scala:244)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:238)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:113)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:376)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)\nURL: https://datasets.imdbws.com/name.basics.tsv.gz\nError while loading title.akas.tsv.gz: (java.lang.SecurityException) Cannot use com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem - local filesystem access is forbidden\n\nJVM stacktrace:\njava.lang.SecurityException\n\tat com.databricks.hadoop.safety.FileSystemCallSiteAllowlist.assertCallSiteIsAllowed(FileSystemCallSiteAllowlist.java:51)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$2(DBUtilsCore.scala:220)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:210)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$1(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:210)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withCpSafetyChecks(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$2(DBUtilsCore.scala:636)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:204)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$1(DBUtilsCore.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:104)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:104)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$recordDbutilsFsOp$6(DBUtilsCore.scala:186)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:186)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.cp(DBUtilsCore.scala:635)\n\tat com.databricks.sql.DbutilsFsSparkConnectBridgeImpl.cp(DbutilsFsSparkConnectBridgeImpl.scala:66)\n\tat com.databricks.sql.managedcatalog.command.CopyDbutilsCommand.run(DbutilsFsCommands.scala:120)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1605)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:701)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:693)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:693)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:693)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:431)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$11(Dataset.scala:244)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:238)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:113)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:376)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)\nURL: https://datasets.imdbws.com/title.akas.tsv.gz\nError while loading title.principals.tsv.gz: (java.lang.SecurityException) Cannot use com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem - local filesystem access is forbidden\n\nJVM stacktrace:\njava.lang.SecurityException\n\tat com.databricks.hadoop.safety.FileSystemCallSiteAllowlist.assertCallSiteIsAllowed(FileSystemCallSiteAllowlist.java:51)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$2(DBUtilsCore.scala:220)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:210)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$1(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:210)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withCpSafetyChecks(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$2(DBUtilsCore.scala:636)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:204)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$1(DBUtilsCore.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:104)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:104)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$recordDbutilsFsOp$6(DBUtilsCore.scala:186)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:186)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.cp(DBUtilsCore.scala:635)\n\tat com.databricks.sql.DbutilsFsSparkConnectBridgeImpl.cp(DbutilsFsSparkConnectBridgeImpl.scala:66)\n\tat com.databricks.sql.managedcatalog.command.CopyDbutilsCommand.run(DbutilsFsCommands.scala:120)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1605)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:701)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:693)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:693)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:693)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:431)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$11(Dataset.scala:244)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:238)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:113)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:376)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)\nURL: https://datasets.imdbws.com/title.principals.tsv.gz\n The following files could not be loaded :\nname.basics.tsv.gz - load it manually to /Volumes/imdb_db/raw_events/imdb_raw\ntitle.akas.tsv.gz - load it manually to /Volumes/imdb_db/raw_events/imdb_raw\ntitle.basics.tsv.gz - load it manually to /Volumes/imdb_db/raw_events/imdb_raw\ntitle.crew.tsv.gz - load it manually to /Volumes/imdb_db/raw_events/imdb_raw\ntitle.episode.tsv.gz - load it manually to /Volumes/imdb_db/raw_events/imdb_raw\ntitle.principals.tsv.gz - load it manually to /Volumes/imdb_db/raw_events/imdb_raw\ntitle.ratings.tsv.gz - load it manually to /Volumes/imdb_db/raw_events/imdb_raw\n"
     ]
    }
   ],
   "source": [
    "# Parallel download\n",
    "failed_files = []\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    results = executor.map(download_and_copy, FILE_NAMES)\n",
    "    failed_files = [res for res in results if res is not None]\n",
    "\n",
    "shutil.rmtree(LOCAL_TMP_PATH, ignore_errors=True)\n",
    "\n",
    "if failed_files:\n",
    "    print(\" The following files could not be loaded :\")\n",
    "    for f in failed_files:\n",
    "        print(f\"{f} - load it manually to {DBFS_PATH}\")\n",
    "else:\n",
    "    print(f\"Files correctly load to {DBFS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdd90936-97df-45fe-91be-d1d9a2faad56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "At this point we see that we can't download the files from Internet so we need to add them by hand to make our notebook able to access it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb29497a-d655-4c2e-a173-adac7875dc6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To correctly load the data in to our project (on Databricks):\n",
    "* Open the catalog (Ctrl + Alt + C or click on the three forms at the right of the widow)\n",
    "* Extend imdb_db, raw_events\n",
    "* On the imdb_raw click on the three vertical dots \n",
    "* Then click on Upload to Volume\n",
    "* Insert the 7 files (gzipped) download from the link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e804f674-32e8-4a83-9c25-7e06f50d40e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reading Gziped TSV files in Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc1f6d1-1456-4bb3-a0be-f98f62c7ba5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_imdb_dataset(file_name: str):\n",
    "    \"\"\"\n",
    "    Read Gzipped TSV file from DBFS into a Sprak DF\n",
    "    \"\"\"\n",
    "    full_path = DBFS_PATH +\"/\"+ file_name\n",
    "    df_name = file_name.replace(\".tsv.gz\", \"\").replace(\".\", \"_\") \n",
    "    \n",
    "    try:\n",
    "        df = (spark.read\n",
    "              .option(\"header\", \"true\")     # Utilise la première ligne comme en-tête\n",
    "              .option(\"delimiter\", \"\\t\")    # Spécifie le délimiteur de tabulation (TSV)\n",
    "              .option(\"inferSchema\", \"true\")# Infère les types de données (pratique pour l'exploration)\n",
    "              .csv(full_path))              # Charge le fichier (Spark gère le .gz)\n",
    "        \n",
    "        # Create a temporary view for easy SQL access\n",
    "        df.createOrReplaceTempView(df_name) \n",
    "        return df_name, df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error while loading {file_name}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c68b23ed-40db-4326-9e91-a6fff14b395f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80f0f45c-0001-4387-b8b0-3b7690bfb999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Spark DF ready: 7\n"
     ]
    }
   ],
   "source": [
    "imdb_dataframes = {}\n",
    "\n",
    "for file_name in FILE_NAMES:\n",
    "    df_name, df = read_imdb_dataset(file_name)\n",
    "    if df_name:\n",
    "        imdb_dataframes[df_name] = df\n",
    "\n",
    "print(f\"Number of Spark DF ready: {len(imdb_dataframes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5456bca-3e5e-4a7c-ad4b-fbf6f131dc3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b5ac0d-5b62-4d18-9515-f9c210dbc173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of name_basics:\nStructType([StructField('nconst', StringType(), True), StructField('primaryName', StringType(), True), StructField('birthYear', StringType(), True), StructField('deathYear', StringType(), True), StructField('primaryProfession', StringType(), True), StructField('knownForTitles', StringType(), True)])\n\n\nSchema of title_akas:\nStructType([StructField('titleId', StringType(), True), StructField('ordering', IntegerType(), True), StructField('title', StringType(), True), StructField('region', StringType(), True), StructField('language', StringType(), True), StructField('types', StringType(), True), StructField('attributes', StringType(), True), StructField('isOriginalTitle', IntegerType(), True)])\n\n\nSchema of title_basics:\nStructType([StructField('tconst', StringType(), True), StructField('titleType', StringType(), True), StructField('primaryTitle', StringType(), True), StructField('originalTitle', StringType(), True), StructField('isAdult', IntegerType(), True), StructField('startYear', StringType(), True), StructField('endYear', StringType(), True), StructField('runtimeMinutes', StringType(), True), StructField('genres', StringType(), True)])\n\n\nSchema of title_crew:\nStructType([StructField('tconst', StringType(), True), StructField('directors', StringType(), True), StructField('writers', StringType(), True)])\n\n\nSchema of title_episode:\nStructType([StructField('tconst', StringType(), True), StructField('parentTconst', StringType(), True), StructField('seasonNumber', StringType(), True), StructField('episodeNumber', StringType(), True)])\n\n\nSchema of title_principals:\nStructType([StructField('tconst', StringType(), True), StructField('ordering', IntegerType(), True), StructField('nconst', StringType(), True), StructField('category', StringType(), True), StructField('job', StringType(), True), StructField('characters', StringType(), True)])\n\n\nSchema of title_ratings:\nStructType([StructField('tconst', StringType(), True), StructField('averageRating', DoubleType(), True), StructField('numVotes', IntegerType(), True)])\n\n\n"
     ]
    }
   ],
   "source": [
    "for df in imdb_dataframes:\n",
    "    print(f\"Schema of {df}:\")\n",
    "    print(imdb_dataframes[df].schema)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b51cc20e-5bb7-430b-9e3b-a580c26a1ca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. How many total people in data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8072eec-716f-4b38-96bf-c2e2e1defb98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people in the dataset: 14925446\n"
     ]
    }
   ],
   "source": [
    "# Calculation of the number of people\n",
    "num_people = imdb_dataframes['name_basics'].count()\n",
    "print(f\"Number of people in the dataset: {num_people}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4015b19e-8f92-419b-b9c3-3619d7797f37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. What is the earliest year of birth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "386c2e92-4cba-4b48-b6bf-f0e108bae83a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's clean birthYear, some value are missing then replaced by '\\N' so we need to cast everything in 'int' to find the exact earliest birthYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f87b73f-f8ca-42ef-8f10-2fa06986cc99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning of birthYear\n",
    "df_names = imdb_dataframes[\"name_basics\"].filter(\n",
    "    F.regexp_extract(\n",
    "        F.col(\"birthYear\"),\n",
    "        \"^[0-9]+$\",\n",
    "        0\n",
    "    ) != \"\"\n",
    ").withColumn(\n",
    "    \"birthYear_int\",\n",
    "    F.col(\"birthYear\").cast(\"int\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "901e4fdb-fb2d-40f9-bda5-99a4e3eb9afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. The earliest year of birth: 4\n"
     ]
    }
   ],
   "source": [
    "earliest_birth_year = df_names.agg(\n",
    "    F.min(\"birthYear_int\")\n",
    ").collect()[0][0]\n",
    "\n",
    "print(f\"3. The earliest year of birth: {earliest_birth_year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb2168b6-170c-48c9-b937-cdcdf0a4c2b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. How many years ago was this person born?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "965ec156-a225-4516-a1b5-89af7d7d3d78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. This person is born 2021 years ago\n"
     ]
    }
   ],
   "source": [
    "current_year = datetime.now().year\n",
    "age = current_year - earliest_birth_year if earliest_birth_year else \"N/A\"\n",
    "\n",
    "print(f\"4. This person is born {age} years ago\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16ab88d-d9b0-49fa-aced-994a4c259055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Using only the data in the data set, determine if this date of birth correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5c4671-6a30-46bd-95bf-83ecdfe3b205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We will take the raw data and just check if the minimal value is the same found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b25c5ec3-edaf-44d4-8bd6-81dee53bfcd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 is NOT the minimum birthYear in the dataset (minimum is 100).\n"
     ]
    }
   ],
   "source": [
    "# Show minimal birthYear values for inspection\n",
    "min_birth_year = imdb_dataframes['name_basics'].agg(\n",
    "    F.min(\"birthYear\")\n",
    ").collect()[0][0]\n",
    "\n",
    "\n",
    "if earliest_birth_year == min_birth_year:\n",
    "    print(f\"{earliest_birth_year} is also the minimum birthYear in the dataset.\")\n",
    "else:\n",
    "    print(f\"{earliest_birth_year} is NOT the minimum birthYear in the dataset (minimum is {min_birth_year}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb3e955-8f2f-4feb-a87b-d9f8aa0094d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>birthYear</th></tr></thead><tbody><tr><td>1561</td></tr><tr><td>1546</td></tr><tr><td>1735</td></tr><tr><td>1628</td></tr><tr><td>2005</td></tr><tr><td>61</td></tr><tr><td>1494</td></tr><tr><td>1788</td></tr><tr><td>2025</td></tr><tr><td>1970</td></tr><tr><td>1994</td></tr><tr><td>1904</td></tr><tr><td>1881</td></tr><tr><td>1814</td></tr><tr><td>1934</td></tr><tr><td>1465</td></tr><tr><td>1945</td></tr><tr><td>1506</td></tr><tr><td>1483</td></tr><tr><td>1806</td></tr><tr><td>1709</td></tr><tr><td>1943</td></tr><tr><td>1966</td></tr><tr><td>1451</td></tr><tr><td>1790</td></tr><tr><td>1563</td></tr><tr><td>1497</td></tr><tr><td>1835</td></tr><tr><td>1507</td></tr><tr><td>1924</td></tr><tr><td>1048</td></tr><tr><td>1947</td></tr><tr><td>1748</td></tr><tr><td>1744</td></tr><tr><td>1717</td></tr><tr><td>1578</td></tr><tr><td>1976</td></tr><tr><td>1992</td></tr><tr><td>1743</td></tr><tr><td>1571</td></tr><tr><td>1265</td></tr><tr><td>1415</td></tr><tr><td>1801</td></tr><tr><td>254</td></tr><tr><td>1676</td></tr><tr><td>1969</td></tr><tr><td>1871</td></tr><tr><td>1639</td></tr><tr><td>1629</td></tr><tr><td>1757</td></tr><tr><td>1813</td></tr><tr><td>1678</td></tr><tr><td>1816</td></tr><tr><td>1822</td></tr><tr><td>1849</td></tr><tr><td>1493</td></tr><tr><td>1239</td></tr><tr><td>1938</td></tr><tr><td>1592</td></tr><tr><td>1674</td></tr><tr><td>1650</td></tr><tr><td>1853</td></tr><tr><td>1929</td></tr><tr><td>1737</td></tr><tr><td>1968</td></tr><tr><td>1984</td></tr><tr><td>1855</td></tr><tr><td>1889</td></tr><tr><td>1782</td></tr><tr><td>1543</td></tr><tr><td>470</td></tr><tr><td>1775</td></tr><tr><td>1965</td></tr><tr><td>1910</td></tr><tr><td>427</td></tr><tr><td>1671</td></tr><tr><td>1975</td></tr><tr><td>1734</td></tr><tr><td>1774</td></tr><tr><td>450</td></tr><tr><td>1722</td></tr><tr><td>1868</td></tr><tr><td>2019</td></tr><tr><td>1657</td></tr><tr><td>1475</td></tr><tr><td>1847</td></tr><tr><td>1830</td></tr><tr><td>2017</td></tr><tr><td>1572</td></tr><tr><td>1917</td></tr><tr><td>1865</td></tr><tr><td>1861</td></tr><tr><td>1550</td></tr><tr><td>2024</td></tr><tr><td>1926</td></tr><tr><td>1859</td></tr><tr><td>1643</td></tr><tr><td>1533</td></tr><tr><td>1783</td></tr><tr><td>1954</td></tr><tr><td>1900</td></tr><tr><td>525</td></tr><tr><td>1225</td></tr><tr><td>1162</td></tr><tr><td>1633</td></tr><tr><td>1828</td></tr><tr><td>1911</td></tr><tr><td>1930</td></tr><tr><td>1899</td></tr><tr><td>1785</td></tr><tr><td>1605</td></tr><tr><td>1736</td></tr><tr><td>1291</td></tr><tr><td>1898</td></tr><tr><td>1983</td></tr><tr><td>1544</td></tr><tr><td>1741</td></tr><tr><td>46</td></tr><tr><td>1623</td></tr><tr><td>1237</td></tr><tr><td>1622</td></tr><tr><td>1778</td></tr><tr><td>1335</td></tr><tr><td>1469</td></tr><tr><td>1773</td></tr><tr><td>1689</td></tr><tr><td>958</td></tr><tr><td>69</td></tr><tr><td>1095</td></tr><tr><td>1879</td></tr><tr><td>1150</td></tr><tr><td>1950</td></tr><tr><td>1698</td></tr><tr><td>1834</td></tr><tr><td>1747</td></tr><tr><td>2014</td></tr><tr><td>1880</td></tr><tr><td>1781</td></tr><tr><td>1707</td></tr><tr><td>1435</td></tr><tr><td>1503</td></tr><tr><td>59</td></tr><tr><td>1610</td></tr><tr><td>1875</td></tr><tr><td>1712</td></tr><tr><td>1749</td></tr><tr><td>1848</td></tr><tr><td>1915</td></tr><tr><td>1827</td></tr><tr><td>1786</td></tr><tr><td>1547</td></tr><tr><td>1745</td></tr><tr><td>1876</td></tr><tr><td>1696</td></tr><tr><td>1810</td></tr><tr><td>1799</td></tr><tr><td>403</td></tr><tr><td>1982</td></tr><tr><td>1870</td></tr><tr><td>1839</td></tr><tr><td>2004</td></tr><tr><td>1725</td></tr><tr><td>1710</td></tr><tr><td>1637</td></tr><tr><td>1492</td></tr><tr><td>1817</td></tr><tr><td>1802</td></tr><tr><td>1771</td></tr><tr><td>1922</td></tr><tr><td>1812</td></tr><tr><td>2012</td></tr><tr><td>1811</td></tr><tr><td>1933</td></tr><tr><td>1999</td></tr><tr><td>1648</td></tr><tr><td>1473</td></tr><tr><td>1940</td></tr><tr><td>1894</td></tr><tr><td>1683</td></tr><tr><td>1601</td></tr><tr><td>1756</td></tr><tr><td>1647</td></tr><tr><td>1423</td></tr><tr><td>1688</td></tr><tr><td>1877</td></tr><tr><td>1659</td></tr><tr><td>1587</td></tr><tr><td>1979</td></tr><tr><td>1942</td></tr><tr><td>1916</td></tr><tr><td>1604</td></tr><tr><td>2000</td></tr><tr><td>1844</td></tr><tr><td>1568</td></tr><tr><td>1450</td></tr><tr><td>1732</td></tr><tr><td>1660</td></tr><tr><td>1694</td></tr><tr><td>1719</td></tr><tr><td>1686</td></tr><tr><td>1857</td></tr><tr><td>1711</td></tr><tr><td>1730</td></tr><tr><td>1440</td></tr><tr><td>1884</td></tr><tr><td>1890</td></tr><tr><td>4</td></tr><tr><td>1655</td></tr><tr><td>2023</td></tr><tr><td>1731</td></tr><tr><td>1715</td></tr><tr><td>1564</td></tr><tr><td>37</td></tr><tr><td>1642</td></tr><tr><td>1398</td></tr><tr><td>1995</td></tr><tr><td>1504</td></tr><tr><td>484</td></tr><tr><td>1599</td></tr><tr><td>1852</td></tr><tr><td>1314</td></tr><tr><td>1575</td></tr><tr><td>1974</td></tr><tr><td>1632</td></tr><tr><td>2013</td></tr><tr><td>1919</td></tr><tr><td>1986</td></tr><tr><td>1410</td></tr><tr><td>1283</td></tr><tr><td>1723</td></tr><tr><td>1477</td></tr><tr><td>1454</td></tr><tr><td>2002</td></tr><tr><td>1713</td></tr><tr><td>1764</td></tr><tr><td>1672</td></tr><tr><td>1542</td></tr><tr><td>1505</td></tr><tr><td>1891</td></tr><tr><td>1997</td></tr><tr><td>1706</td></tr><tr><td>1474</td></tr><tr><td>1921</td></tr><tr><td>1893</td></tr><tr><td>1897</td></tr><tr><td>1524</td></tr><tr><td>1515</td></tr><tr><td>1750</td></tr><tr><td>1589</td></tr><tr><td>1207</td></tr><tr><td>1330</td></tr><tr><td>1874</td></tr><tr><td>1920</td></tr><tr><td>1978</td></tr><tr><td>1761</td></tr><tr><td>1905</td></tr><tr><td>1471</td></tr><tr><td>1758</td></tr><tr><td>1873</td></tr><tr><td>1697</td></tr><tr><td>1586</td></tr><tr><td>1789</td></tr><tr><td>1598</td></tr><tr><td>1895</td></tr><tr><td>1913</td></tr><tr><td>1685</td></tr><tr><td>1946</td></tr><tr><td>1618</td></tr><tr><td>1621</td></tr><tr><td>1738</td></tr><tr><td>1566</td></tr><tr><td>1746</td></tr><tr><td>1666</td></tr><tr><td>1840</td></tr><tr><td>1787</td></tr><tr><td>1912</td></tr><tr><td>1989</td></tr><tr><td>1498</td></tr><tr><td>1833</td></tr><tr><td>1792</td></tr><tr><td>1193</td></tr><tr><td>1675</td></tr><tr><td>1728</td></tr><tr><td>1733</td></tr><tr><td>1681</td></tr><tr><td>1831</td></tr><tr><td>1414</td></tr><tr><td>1819</td></tr><tr><td>1939</td></tr><tr><td>1962</td></tr><tr><td>1759</td></tr><tr><td>1862</td></tr><tr><td>2010</td></tr><tr><td>1665</td></tr><tr><td>1851</td></tr><tr><td>1987</td></tr><tr><td>1402</td></tr><tr><td>1981</td></tr><tr><td>1996</td></tr><tr><td>496</td></tr><tr><td>1953</td></tr><tr><td>1608</td></tr><tr><td>1772</td></tr><tr><td>1693</td></tr><tr><td>1937</td></tr><tr><td>1914</td></tr><tr><td>430</td></tr><tr><td>1646</td></tr><tr><td>1964</td></tr><tr><td>1985</td></tr><tr><td>1800</td></tr><tr><td>2022</td></tr><tr><td>1636</td></tr><tr><td>1641</td></tr><tr><td>1955</td></tr><tr><td>1918</td></tr><tr><td>1762</td></tr><tr><td>1796</td></tr><tr><td>1867</td></tr><tr><td>570</td></tr><tr><td>1751</td></tr><tr><td>1720</td></tr><tr><td>1703</td></tr><tr><td>2015</td></tr><tr><td>1577</td></tr><tr><td>1489</td></tr><tr><td>2009</td></tr><tr><td>1667</td></tr><tr><td>1420</td></tr><tr><td>1854</td></tr><tr><td>1863</td></tr><tr><td>1422</td></tr><tr><td>1668</td></tr><tr><td>1973</td></tr><tr><td>1886</td></tr><tr><td>1815</td></tr><tr><td>1593</td></tr><tr><td>1769</td></tr><tr><td>1988</td></tr><tr><td>1491</td></tr><tr><td>1944</td></tr><tr><td>1776</td></tr><tr><td>1527</td></tr><tr><td>1927</td></tr><tr><td>1990</td></tr><tr><td>1296</td></tr><tr><td>1957</td></tr><tr><td>1540</td></tr><tr><td>1616</td></tr><tr><td>1412</td></tr><tr><td>1935</td></tr><tr><td>1780</td></tr><tr><td>1998</td></tr><tr><td>1928</td></tr><tr><td>1765</td></tr><tr><td>1700</td></tr><tr><td>1343</td></tr><tr><td>1254</td></tr><tr><td>1638</td></tr><tr><td>1824</td></tr><tr><td>1832</td></tr><tr><td>354</td></tr><tr><td>1753</td></tr><tr><td>1413</td></tr><tr><td>604</td></tr><tr><td>390</td></tr><tr><td>1687</td></tr><tr><td>1807</td></tr><tr><td>1480</td></tr><tr><td>1967</td></tr><tr><td>1825</td></tr><tr><td>1803</td></tr><tr><td>850</td></tr><tr><td>2007</td></tr><tr><td>1754</td></tr><tr><td>1669</td></tr><tr><td>1630</td></tr><tr><td>1727</td></tr><tr><td>1948</td></tr><tr><td>1850</td></tr><tr><td>1724</td></tr><tr><td>1896</td></tr><tr><td>1818</td></tr><tr><td>1596</td></tr><tr><td>1908</td></tr><tr><td>\\N</td></tr><tr><td>1866</td></tr><tr><td>551</td></tr><tr><td>1500</td></tr><tr><td>1923</td></tr><tr><td>1936</td></tr><tr><td>1431</td></tr><tr><td>1670</td></tr><tr><td>1350</td></tr><tr><td>1791</td></tr><tr><td>1766</td></tr><tr><td>1820</td></tr><tr><td>1485</td></tr><tr><td>1261</td></tr><tr><td>1580</td></tr><tr><td>1530</td></tr><tr><td>1760</td></tr><tr><td>1579</td></tr><tr><td>1963</td></tr><tr><td>1644</td></tr><tr><td>1808</td></tr><tr><td>1903</td></tr><tr><td>1784</td></tr><tr><td>1767</td></tr><tr><td>1845</td></tr><tr><td>1695</td></tr><tr><td>1602</td></tr><tr><td>2003</td></tr><tr><td>1567</td></tr><tr><td>1658</td></tr><tr><td>1809</td></tr><tr><td>1843</td></tr><tr><td>1768</td></tr><tr><td>1883</td></tr><tr><td>1836</td></tr><tr><td>1972</td></tr><tr><td>1606</td></tr><tr><td>1635</td></tr><tr><td>1520</td></tr><tr><td>1961</td></tr><tr><td>1823</td></tr><tr><td>1654</td></tr><tr><td>1701</td></tr><tr><td>1888</td></tr><tr><td>1805</td></tr><tr><td>1656</td></tr><tr><td>1770</td></tr><tr><td>384</td></tr><tr><td>1663</td></tr><tr><td>2016</td></tr><tr><td>1664</td></tr><tr><td>2006</td></tr><tr><td>1486</td></tr><tr><td>949</td></tr><tr><td>1878</td></tr><tr><td>1584</td></tr><tr><td>1603</td></tr><tr><td>1959</td></tr><tr><td>1619</td></tr><tr><td>1595</td></tr><tr><td>1626</td></tr><tr><td>1909</td></tr><tr><td>1684</td></tr><tr><td>1991</td></tr><tr><td>1386</td></tr><tr><td>1313</td></tr><tr><td>973</td></tr><tr><td>1558</td></tr><tr><td>2018</td></tr><tr><td>1892</td></tr><tr><td>1682</td></tr><tr><td>1739</td></tr><tr><td>2021</td></tr><tr><td>1941</td></tr><tr><td>1993</td></tr><tr><td>1631</td></tr><tr><td>1560</td></tr><tr><td>1525</td></tr><tr><td>1569</td></tr><tr><td>1552</td></tr><tr><td>1549</td></tr><tr><td>1864</td></tr><tr><td>1508</td></tr><tr><td>100</td></tr><tr><td>1180</td></tr><tr><td>1931</td></tr><tr><td>1958</td></tr><tr><td>1821</td></tr><tr><td>1130</td></tr><tr><td>1511</td></tr><tr><td>1221</td></tr><tr><td>1869</td></tr><tr><td>1099</td></tr><tr><td>1581</td></tr><tr><td>1960</td></tr><tr><td>1826</td></tr><tr><td>1952</td></tr><tr><td>1755</td></tr><tr><td>70</td></tr><tr><td>1829</td></tr><tr><td>1838</td></tr><tr><td>1837</td></tr><tr><td>1860</td></tr><tr><td>43</td></tr><tr><td>1640</td></tr><tr><td>1971</td></tr><tr><td>1532</td></tr><tr><td>1562</td></tr><tr><td>1858</td></tr><tr><td>1729</td></tr><tr><td>1716</td></tr><tr><td>1885</td></tr><tr><td>1653</td></tr><tr><td>1777</td></tr><tr><td>1872</td></tr><tr><td>1763</td></tr><tr><td>1901</td></tr><tr><td>1906</td></tr><tr><td>1902</td></tr><tr><td>1925</td></tr><tr><td>1588</td></tr><tr><td>1597</td></tr><tr><td>1932</td></tr><tr><td>1846</td></tr><tr><td>20</td></tr><tr><td>1714</td></tr><tr><td>1842</td></tr><tr><td>1627</td></tr><tr><td>2011</td></tr><tr><td>1726</td></tr><tr><td>1510</td></tr><tr><td>1980</td></tr><tr><td>620</td></tr><tr><td>163</td></tr><tr><td>1793</td></tr><tr><td>1582</td></tr><tr><td>1625</td></tr><tr><td>1882</td></tr><tr><td>2020</td></tr><tr><td>2008</td></tr><tr><td>1887</td></tr><tr><td>1797</td></tr><tr><td>1740</td></tr><tr><td>1600</td></tr><tr><td>1798</td></tr><tr><td>1452</td></tr><tr><td>1841</td></tr><tr><td>1779</td></tr><tr><td>1455</td></tr><tr><td>1856</td></tr><tr><td>1907</td></tr><tr><td>1529</td></tr><tr><td>1652</td></tr><tr><td>1977</td></tr><tr><td>1956</td></tr><tr><td>1634</td></tr><tr><td>1585</td></tr><tr><td>1449</td></tr><tr><td>1951</td></tr><tr><td>1673</td></tr><tr><td>1645</td></tr><tr><td>1795</td></tr><tr><td>2001</td></tr><tr><td>1794</td></tr><tr><td>1804</td></tr><tr><td>1232</td></tr><tr><td>95</td></tr><tr><td>1098</td></tr><tr><td>1677</td></tr><tr><td>1949</td></tr><tr><td>1752</td></tr><tr><td>1583</td></tr><tr><td>770</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1561"
        ],
        [
         "1546"
        ],
        [
         "1735"
        ],
        [
         "1628"
        ],
        [
         "2005"
        ],
        [
         "61"
        ],
        [
         "1494"
        ],
        [
         "1788"
        ],
        [
         "2025"
        ],
        [
         "1970"
        ],
        [
         "1994"
        ],
        [
         "1904"
        ],
        [
         "1881"
        ],
        [
         "1814"
        ],
        [
         "1934"
        ],
        [
         "1465"
        ],
        [
         "1945"
        ],
        [
         "1506"
        ],
        [
         "1483"
        ],
        [
         "1806"
        ],
        [
         "1709"
        ],
        [
         "1943"
        ],
        [
         "1966"
        ],
        [
         "1451"
        ],
        [
         "1790"
        ],
        [
         "1563"
        ],
        [
         "1497"
        ],
        [
         "1835"
        ],
        [
         "1507"
        ],
        [
         "1924"
        ],
        [
         "1048"
        ],
        [
         "1947"
        ],
        [
         "1748"
        ],
        [
         "1744"
        ],
        [
         "1717"
        ],
        [
         "1578"
        ],
        [
         "1976"
        ],
        [
         "1992"
        ],
        [
         "1743"
        ],
        [
         "1571"
        ],
        [
         "1265"
        ],
        [
         "1415"
        ],
        [
         "1801"
        ],
        [
         "254"
        ],
        [
         "1676"
        ],
        [
         "1969"
        ],
        [
         "1871"
        ],
        [
         "1639"
        ],
        [
         "1629"
        ],
        [
         "1757"
        ],
        [
         "1813"
        ],
        [
         "1678"
        ],
        [
         "1816"
        ],
        [
         "1822"
        ],
        [
         "1849"
        ],
        [
         "1493"
        ],
        [
         "1239"
        ],
        [
         "1938"
        ],
        [
         "1592"
        ],
        [
         "1674"
        ],
        [
         "1650"
        ],
        [
         "1853"
        ],
        [
         "1929"
        ],
        [
         "1737"
        ],
        [
         "1968"
        ],
        [
         "1984"
        ],
        [
         "1855"
        ],
        [
         "1889"
        ],
        [
         "1782"
        ],
        [
         "1543"
        ],
        [
         "470"
        ],
        [
         "1775"
        ],
        [
         "1965"
        ],
        [
         "1910"
        ],
        [
         "427"
        ],
        [
         "1671"
        ],
        [
         "1975"
        ],
        [
         "1734"
        ],
        [
         "1774"
        ],
        [
         "450"
        ],
        [
         "1722"
        ],
        [
         "1868"
        ],
        [
         "2019"
        ],
        [
         "1657"
        ],
        [
         "1475"
        ],
        [
         "1847"
        ],
        [
         "1830"
        ],
        [
         "2017"
        ],
        [
         "1572"
        ],
        [
         "1917"
        ],
        [
         "1865"
        ],
        [
         "1861"
        ],
        [
         "1550"
        ],
        [
         "2024"
        ],
        [
         "1926"
        ],
        [
         "1859"
        ],
        [
         "1643"
        ],
        [
         "1533"
        ],
        [
         "1783"
        ],
        [
         "1954"
        ],
        [
         "1900"
        ],
        [
         "525"
        ],
        [
         "1225"
        ],
        [
         "1162"
        ],
        [
         "1633"
        ],
        [
         "1828"
        ],
        [
         "1911"
        ],
        [
         "1930"
        ],
        [
         "1899"
        ],
        [
         "1785"
        ],
        [
         "1605"
        ],
        [
         "1736"
        ],
        [
         "1291"
        ],
        [
         "1898"
        ],
        [
         "1983"
        ],
        [
         "1544"
        ],
        [
         "1741"
        ],
        [
         "46"
        ],
        [
         "1623"
        ],
        [
         "1237"
        ],
        [
         "1622"
        ],
        [
         "1778"
        ],
        [
         "1335"
        ],
        [
         "1469"
        ],
        [
         "1773"
        ],
        [
         "1689"
        ],
        [
         "958"
        ],
        [
         "69"
        ],
        [
         "1095"
        ],
        [
         "1879"
        ],
        [
         "1150"
        ],
        [
         "1950"
        ],
        [
         "1698"
        ],
        [
         "1834"
        ],
        [
         "1747"
        ],
        [
         "2014"
        ],
        [
         "1880"
        ],
        [
         "1781"
        ],
        [
         "1707"
        ],
        [
         "1435"
        ],
        [
         "1503"
        ],
        [
         "59"
        ],
        [
         "1610"
        ],
        [
         "1875"
        ],
        [
         "1712"
        ],
        [
         "1749"
        ],
        [
         "1848"
        ],
        [
         "1915"
        ],
        [
         "1827"
        ],
        [
         "1786"
        ],
        [
         "1547"
        ],
        [
         "1745"
        ],
        [
         "1876"
        ],
        [
         "1696"
        ],
        [
         "1810"
        ],
        [
         "1799"
        ],
        [
         "403"
        ],
        [
         "1982"
        ],
        [
         "1870"
        ],
        [
         "1839"
        ],
        [
         "2004"
        ],
        [
         "1725"
        ],
        [
         "1710"
        ],
        [
         "1637"
        ],
        [
         "1492"
        ],
        [
         "1817"
        ],
        [
         "1802"
        ],
        [
         "1771"
        ],
        [
         "1922"
        ],
        [
         "1812"
        ],
        [
         "2012"
        ],
        [
         "1811"
        ],
        [
         "1933"
        ],
        [
         "1999"
        ],
        [
         "1648"
        ],
        [
         "1473"
        ],
        [
         "1940"
        ],
        [
         "1894"
        ],
        [
         "1683"
        ],
        [
         "1601"
        ],
        [
         "1756"
        ],
        [
         "1647"
        ],
        [
         "1423"
        ],
        [
         "1688"
        ],
        [
         "1877"
        ],
        [
         "1659"
        ],
        [
         "1587"
        ],
        [
         "1979"
        ],
        [
         "1942"
        ],
        [
         "1916"
        ],
        [
         "1604"
        ],
        [
         "2000"
        ],
        [
         "1844"
        ],
        [
         "1568"
        ],
        [
         "1450"
        ],
        [
         "1732"
        ],
        [
         "1660"
        ],
        [
         "1694"
        ],
        [
         "1719"
        ],
        [
         "1686"
        ],
        [
         "1857"
        ],
        [
         "1711"
        ],
        [
         "1730"
        ],
        [
         "1440"
        ],
        [
         "1884"
        ],
        [
         "1890"
        ],
        [
         "4"
        ],
        [
         "1655"
        ],
        [
         "2023"
        ],
        [
         "1731"
        ],
        [
         "1715"
        ],
        [
         "1564"
        ],
        [
         "37"
        ],
        [
         "1642"
        ],
        [
         "1398"
        ],
        [
         "1995"
        ],
        [
         "1504"
        ],
        [
         "484"
        ],
        [
         "1599"
        ],
        [
         "1852"
        ],
        [
         "1314"
        ],
        [
         "1575"
        ],
        [
         "1974"
        ],
        [
         "1632"
        ],
        [
         "2013"
        ],
        [
         "1919"
        ],
        [
         "1986"
        ],
        [
         "1410"
        ],
        [
         "1283"
        ],
        [
         "1723"
        ],
        [
         "1477"
        ],
        [
         "1454"
        ],
        [
         "2002"
        ],
        [
         "1713"
        ],
        [
         "1764"
        ],
        [
         "1672"
        ],
        [
         "1542"
        ],
        [
         "1505"
        ],
        [
         "1891"
        ],
        [
         "1997"
        ],
        [
         "1706"
        ],
        [
         "1474"
        ],
        [
         "1921"
        ],
        [
         "1893"
        ],
        [
         "1897"
        ],
        [
         "1524"
        ],
        [
         "1515"
        ],
        [
         "1750"
        ],
        [
         "1589"
        ],
        [
         "1207"
        ],
        [
         "1330"
        ],
        [
         "1874"
        ],
        [
         "1920"
        ],
        [
         "1978"
        ],
        [
         "1761"
        ],
        [
         "1905"
        ],
        [
         "1471"
        ],
        [
         "1758"
        ],
        [
         "1873"
        ],
        [
         "1697"
        ],
        [
         "1586"
        ],
        [
         "1789"
        ],
        [
         "1598"
        ],
        [
         "1895"
        ],
        [
         "1913"
        ],
        [
         "1685"
        ],
        [
         "1946"
        ],
        [
         "1618"
        ],
        [
         "1621"
        ],
        [
         "1738"
        ],
        [
         "1566"
        ],
        [
         "1746"
        ],
        [
         "1666"
        ],
        [
         "1840"
        ],
        [
         "1787"
        ],
        [
         "1912"
        ],
        [
         "1989"
        ],
        [
         "1498"
        ],
        [
         "1833"
        ],
        [
         "1792"
        ],
        [
         "1193"
        ],
        [
         "1675"
        ],
        [
         "1728"
        ],
        [
         "1733"
        ],
        [
         "1681"
        ],
        [
         "1831"
        ],
        [
         "1414"
        ],
        [
         "1819"
        ],
        [
         "1939"
        ],
        [
         "1962"
        ],
        [
         "1759"
        ],
        [
         "1862"
        ],
        [
         "2010"
        ],
        [
         "1665"
        ],
        [
         "1851"
        ],
        [
         "1987"
        ],
        [
         "1402"
        ],
        [
         "1981"
        ],
        [
         "1996"
        ],
        [
         "496"
        ],
        [
         "1953"
        ],
        [
         "1608"
        ],
        [
         "1772"
        ],
        [
         "1693"
        ],
        [
         "1937"
        ],
        [
         "1914"
        ],
        [
         "430"
        ],
        [
         "1646"
        ],
        [
         "1964"
        ],
        [
         "1985"
        ],
        [
         "1800"
        ],
        [
         "2022"
        ],
        [
         "1636"
        ],
        [
         "1641"
        ],
        [
         "1955"
        ],
        [
         "1918"
        ],
        [
         "1762"
        ],
        [
         "1796"
        ],
        [
         "1867"
        ],
        [
         "570"
        ],
        [
         "1751"
        ],
        [
         "1720"
        ],
        [
         "1703"
        ],
        [
         "2015"
        ],
        [
         "1577"
        ],
        [
         "1489"
        ],
        [
         "2009"
        ],
        [
         "1667"
        ],
        [
         "1420"
        ],
        [
         "1854"
        ],
        [
         "1863"
        ],
        [
         "1422"
        ],
        [
         "1668"
        ],
        [
         "1973"
        ],
        [
         "1886"
        ],
        [
         "1815"
        ],
        [
         "1593"
        ],
        [
         "1769"
        ],
        [
         "1988"
        ],
        [
         "1491"
        ],
        [
         "1944"
        ],
        [
         "1776"
        ],
        [
         "1527"
        ],
        [
         "1927"
        ],
        [
         "1990"
        ],
        [
         "1296"
        ],
        [
         "1957"
        ],
        [
         "1540"
        ],
        [
         "1616"
        ],
        [
         "1412"
        ],
        [
         "1935"
        ],
        [
         "1780"
        ],
        [
         "1998"
        ],
        [
         "1928"
        ],
        [
         "1765"
        ],
        [
         "1700"
        ],
        [
         "1343"
        ],
        [
         "1254"
        ],
        [
         "1638"
        ],
        [
         "1824"
        ],
        [
         "1832"
        ],
        [
         "354"
        ],
        [
         "1753"
        ],
        [
         "1413"
        ],
        [
         "604"
        ],
        [
         "390"
        ],
        [
         "1687"
        ],
        [
         "1807"
        ],
        [
         "1480"
        ],
        [
         "1967"
        ],
        [
         "1825"
        ],
        [
         "1803"
        ],
        [
         "850"
        ],
        [
         "2007"
        ],
        [
         "1754"
        ],
        [
         "1669"
        ],
        [
         "1630"
        ],
        [
         "1727"
        ],
        [
         "1948"
        ],
        [
         "1850"
        ],
        [
         "1724"
        ],
        [
         "1896"
        ],
        [
         "1818"
        ],
        [
         "1596"
        ],
        [
         "1908"
        ],
        [
         "\\N"
        ],
        [
         "1866"
        ],
        [
         "551"
        ],
        [
         "1500"
        ],
        [
         "1923"
        ],
        [
         "1936"
        ],
        [
         "1431"
        ],
        [
         "1670"
        ],
        [
         "1350"
        ],
        [
         "1791"
        ],
        [
         "1766"
        ],
        [
         "1820"
        ],
        [
         "1485"
        ],
        [
         "1261"
        ],
        [
         "1580"
        ],
        [
         "1530"
        ],
        [
         "1760"
        ],
        [
         "1579"
        ],
        [
         "1963"
        ],
        [
         "1644"
        ],
        [
         "1808"
        ],
        [
         "1903"
        ],
        [
         "1784"
        ],
        [
         "1767"
        ],
        [
         "1845"
        ],
        [
         "1695"
        ],
        [
         "1602"
        ],
        [
         "2003"
        ],
        [
         "1567"
        ],
        [
         "1658"
        ],
        [
         "1809"
        ],
        [
         "1843"
        ],
        [
         "1768"
        ],
        [
         "1883"
        ],
        [
         "1836"
        ],
        [
         "1972"
        ],
        [
         "1606"
        ],
        [
         "1635"
        ],
        [
         "1520"
        ],
        [
         "1961"
        ],
        [
         "1823"
        ],
        [
         "1654"
        ],
        [
         "1701"
        ],
        [
         "1888"
        ],
        [
         "1805"
        ],
        [
         "1656"
        ],
        [
         "1770"
        ],
        [
         "384"
        ],
        [
         "1663"
        ],
        [
         "2016"
        ],
        [
         "1664"
        ],
        [
         "2006"
        ],
        [
         "1486"
        ],
        [
         "949"
        ],
        [
         "1878"
        ],
        [
         "1584"
        ],
        [
         "1603"
        ],
        [
         "1959"
        ],
        [
         "1619"
        ],
        [
         "1595"
        ],
        [
         "1626"
        ],
        [
         "1909"
        ],
        [
         "1684"
        ],
        [
         "1991"
        ],
        [
         "1386"
        ],
        [
         "1313"
        ],
        [
         "973"
        ],
        [
         "1558"
        ],
        [
         "2018"
        ],
        [
         "1892"
        ],
        [
         "1682"
        ],
        [
         "1739"
        ],
        [
         "2021"
        ],
        [
         "1941"
        ],
        [
         "1993"
        ],
        [
         "1631"
        ],
        [
         "1560"
        ],
        [
         "1525"
        ],
        [
         "1569"
        ],
        [
         "1552"
        ],
        [
         "1549"
        ],
        [
         "1864"
        ],
        [
         "1508"
        ],
        [
         "100"
        ],
        [
         "1180"
        ],
        [
         "1931"
        ],
        [
         "1958"
        ],
        [
         "1821"
        ],
        [
         "1130"
        ],
        [
         "1511"
        ],
        [
         "1221"
        ],
        [
         "1869"
        ],
        [
         "1099"
        ],
        [
         "1581"
        ],
        [
         "1960"
        ],
        [
         "1826"
        ],
        [
         "1952"
        ],
        [
         "1755"
        ],
        [
         "70"
        ],
        [
         "1829"
        ],
        [
         "1838"
        ],
        [
         "1837"
        ],
        [
         "1860"
        ],
        [
         "43"
        ],
        [
         "1640"
        ],
        [
         "1971"
        ],
        [
         "1532"
        ],
        [
         "1562"
        ],
        [
         "1858"
        ],
        [
         "1729"
        ],
        [
         "1716"
        ],
        [
         "1885"
        ],
        [
         "1653"
        ],
        [
         "1777"
        ],
        [
         "1872"
        ],
        [
         "1763"
        ],
        [
         "1901"
        ],
        [
         "1906"
        ],
        [
         "1902"
        ],
        [
         "1925"
        ],
        [
         "1588"
        ],
        [
         "1597"
        ],
        [
         "1932"
        ],
        [
         "1846"
        ],
        [
         "20"
        ],
        [
         "1714"
        ],
        [
         "1842"
        ],
        [
         "1627"
        ],
        [
         "2011"
        ],
        [
         "1726"
        ],
        [
         "1510"
        ],
        [
         "1980"
        ],
        [
         "620"
        ],
        [
         "163"
        ],
        [
         "1793"
        ],
        [
         "1582"
        ],
        [
         "1625"
        ],
        [
         "1882"
        ],
        [
         "2020"
        ],
        [
         "2008"
        ],
        [
         "1887"
        ],
        [
         "1797"
        ],
        [
         "1740"
        ],
        [
         "1600"
        ],
        [
         "1798"
        ],
        [
         "1452"
        ],
        [
         "1841"
        ],
        [
         "1779"
        ],
        [
         "1455"
        ],
        [
         "1856"
        ],
        [
         "1907"
        ],
        [
         "1529"
        ],
        [
         "1652"
        ],
        [
         "1977"
        ],
        [
         "1956"
        ],
        [
         "1634"
        ],
        [
         "1585"
        ],
        [
         "1449"
        ],
        [
         "1951"
        ],
        [
         "1673"
        ],
        [
         "1645"
        ],
        [
         "1795"
        ],
        [
         "2001"
        ],
        [
         "1794"
        ],
        [
         "1804"
        ],
        [
         "1232"
        ],
        [
         "95"
        ],
        [
         "1098"
        ],
        [
         "1677"
        ],
        [
         "1949"
        ],
        [
         "1752"
        ],
        [
         "1583"
        ],
        [
         "770"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "birthYear",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(imdb_dataframes['name_basics'].select(\"birthYear\").distinct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "668938eb-3bc2-4263-8415-bcb6fe00ea7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We have a different number from what we calculate first. But it's not really surprising it's because the dataset from internet is not clean so if some data is missing or is simply not clean it might not find it as minimal.\n",
    "By running a quick display we can see that some date are below 100 so the result without cleaning might be the correct one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cdbd2fd-02c4-4c7c-9384-b5f488d882a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Explain the reasoning for the answer in a code comment or new markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb36c65e-78ad-4949-b829-9e3f7cbe11e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Give a good explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c70c2a5-5782-445e-9b80-59ebeb36d674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. What is the most recent date of birth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80cccb19-e5cf-49d3-ac47-9edc001cee97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. The most recent year of birth: 2025\n"
     ]
    }
   ],
   "source": [
    "earliest_birth_year = df_names.agg(\n",
    "    F.max(\"birthYear_int\")\n",
    ").collect()[0][0]\n",
    "\n",
    "print(f\"7. The most recent year of birth: {earliest_birth_year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac6f0e5-a382-4386-8ab1-c045005ad266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. What percentage of the people do not have a listed date of birth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6699c3e-b838-4f5f-9ab8-18f2fd83f8f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. Percentage of people without birthday listed is : 95.57%\n"
     ]
    }
   ],
   "source": [
    "total_rows = imdb_dataframes['name_basics'].count()\n",
    "null_birth_rows = imdb_dataframes['name_basics'].filter(\n",
    "    (F.col(\"birthYear\").isNull()) | (F.col(\"birthYear\") == \"\\\\N\")\n",
    ").count()\n",
    "\n",
    "percentage_no_birth_date = (null_birth_rows / total_rows) * 100 if total_rows > 0 else 0\n",
    "\n",
    "print(f\"8. Percentage of people without birthday listed is : {percentage_no_birth_date:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac1f1c78-ceeb-4566-87a2-be9b725d08d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_titles_clean = imdb_dataframes['title_basics'].filter(\n",
    "    (F.col(\"runtimeMinutes\") != \"\\\\N\") &\n",
    "    (F.col(\"runtimeMinutes\").isNotNull()) &\n",
    "    (F.col(\"startYear\") != \"\\\\N\") &\n",
    "    (F.col(\"startYear\").isNotNull())\n",
    ").withColumn(\n",
    "    \"runtimeMinutes_int\",\n",
    "    F.expr(\"try_cast(runtimeMinutes as int)\")\n",
    ").withColumn(\n",
    "    \"startYear_int\",\n",
    "    F.expr(\"try_cast(startYear as int)\")\n",
    ").filter(\n",
    "    (F.col(\"runtimeMinutes_int\").isNotNull()) &\n",
    "    (F.col(\"startYear_int\") > 1900)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a816bec-052b-4036-af94-9eafe313b29c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. What is the length of the longest \"short\" after 1900?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cfbe51e-fad8-4cfa-9ad4-63f86972283b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9. The longest short after 1900: 1311 minutes\n"
     ]
    }
   ],
   "source": [
    "# Find the longest 'short' runtime\n",
    "longest_short_runtime = df_titles_clean.filter(\n",
    "    F.col(\"titleType\") == \"short\"\n",
    ").agg(\n",
    "    F.max(\"runtimeMinutes_int\")\n",
    ").collect()[0][0]\n",
    "\n",
    "print(f\"9. The longest short after 1900: {longest_short_runtime} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d91c515d-d373-41ec-8524-5b9f6b5f266f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. What is the length of the shortest \"movie\" after 1900? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b33f62-e6ff-4757-9293-9c402206440a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10. The shortest movie after 1900 is : 1 minutes\n"
     ]
    }
   ],
   "source": [
    "shortest_movie_runtime = df_titles_clean.filter(\n",
    "    F.col(\"titleType\") == \"movie\"\n",
    ").agg(\n",
    "    F.min(\"runtimeMinutes_int\")\n",
    ").collect()[0][0]\n",
    "\n",
    "print(f\"10. The shortest movie after 1900 is : {shortest_movie_runtime} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f1af646-e904-40f2-95b5-565549521a18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "11. List of all of the genres represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "132ce80b-52f0-41b2-9b11-6fc236a46224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11. List of all represented gender: ['Adventure', 'Horror', 'Thriller', 'History', 'Romance', 'Western', 'Comedy', 'Fantasy', 'Film-Noir', 'Adult', 'Short', 'Crime', 'Animation', 'Talk-Show', 'Mystery', 'Biography', 'Reality-TV', 'Sci-Fi', 'Game-Show', 'Family', 'War', 'Documentary', 'Musical', 'Drama', 'News', 'Music', 'Action', 'Sport']\n"
     ]
    }
   ],
   "source": [
    "all_genres = imdb_dataframes['title_basics'].select(\n",
    "    F.explode(F.split(F.col(\"genres\"), \",\"))\n",
    ").distinct().filter(F.col(\"col\") != \"\\\\N\").collect()\n",
    "\n",
    "genre_list = [row['col'] for row in all_genres]\n",
    "\n",
    "print(f\"11. List of all represented gender: {genre_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04ad6a24-8c20-459d-b91a-b418dde24fac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_rated_titles = imdb_dataframes['title_basics'].alias(\"b\").join(\n",
    "    imdb_dataframes['title_ratings'].alias(\"r\"),\n",
    "    F.col(\"b.tconst\") == F.col(\"r.tconst\"),\n",
    "    \"inner\"\n",
    ").filter(\n",
    "    (F.col(\"b.titleType\") == \"movie\") & \n",
    "    (F.col(\"b.genres\").like(\"%Comedy%\")) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb103682-e951-4fa4-8820-f819f14ae374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "12. What is the highest rated comedy \"movie\" in the dataset? Note, if there is a tie, the tie shall be broken by the movie with the most votes .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639b9c31-bd2e-432e-8017-8976560343af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10. The best comic movie is : O La La (tconst: tt8458418)\n"
     ]
    }
   ],
   "source": [
    "best_comedy_movie = df_rated_titles.orderBy(\n",
    "    F.col(\"averageRating\").desc(), \n",
    "    F.col(\"numVotes\").desc()\n",
    ").select(\"b.tconst\", \"b.primaryTitle\").limit(1).collect()\n",
    "\n",
    "if best_comedy_movie:\n",
    "    best_tconst = best_comedy_movie[0]['tconst']\n",
    "    best_title = best_comedy_movie[0]['primaryTitle']\n",
    "    print(f\"10. The best comic movie is : {best_title} (tconst: {best_tconst})\")\n",
    "else:\n",
    "    best_tconst = None\n",
    "    print(\"12. No comic movie were found with the grade.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66beb7ca-88ab-4097-b53b-52b07ca9c47c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "13. Who was the director of the movie?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52754823-aa54-4a1c-a547-9bc99555feda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n13. Director of 'O La La': Sripad Pai\n"
     ]
    }
   ],
   "source": [
    "if best_tconst:\n",
    "    # 1. Join title_crew and name_basics\n",
    "    df_crew_names = imdb_dataframes['title_crew'].alias(\"tc\").join(\n",
    "        imdb_dataframes['name_basics'].alias(\"nb\"),\n",
    "        F.split(F.col(\"tc.directors\"), \",\").contains(F.col(\"nb.nconst\")),\n",
    "        \"inner\"\n",
    "    ).filter(F.col(\"tc.tconst\") == best_tconst)\n",
    "\n",
    "    # 2. Get director names\n",
    "    director_nconsts_row = imdb_dataframes['title_crew'].filter(F.col(\"tconst\") == best_tconst).select(\"directors\").collect()\n",
    "\n",
    "    if director_nconsts_row and director_nconsts_row[0]['directors'] != '\\\\N':\n",
    "        director_nconsts = director_nconsts_row[0]['directors'].split(',')\n",
    "        \n",
    "        directors_df = imdb_dataframes['name_basics'].filter(F.col(\"nconst\").isin(director_nconsts)).select(\"primaryName\").collect()\n",
    "        director_names = [row['primaryName'] for row in directors_df]\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        print(f\"13. Director of '{best_title}': {', '.join(director_names)}\")\n",
    "    else:\n",
    "        print(\"13. Director not listed\")\n",
    "else:\n",
    "    print(\"13. Director not found because no movie was found question 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0265eb56-3474-4f7f-aa79-97fcc33a95e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "14. List, if any, the alternate titles for the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ce00f87-8a0a-465f-9fe4-8901e6e4dfd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n14. Alternate title for the movie 'O La La':\n - O La La\n"
     ]
    }
   ],
   "source": [
    "if best_tconst:\n",
    "    # Join title_akas (alternate titles) using titleId\n",
    "    alternate_titles = imdb_dataframes['title_akas'].filter(\n",
    "        F.col(\"titleId\") == best_tconst\n",
    "    ).select(\"title\").distinct().collect()\n",
    "    \n",
    "    aka_titles = [row['title'] for row in alternate_titles]\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"14. Alternate title for the movie '{best_title}':\")\n",
    "    if aka_titles:\n",
    "        for title in aka_titles:\n",
    "            print(f\" - {title}\")\n",
    "    else:\n",
    "        print(\" - No title alternate found\")\n",
    "else:\n",
    "    print(\"14. Alternate titles not found because no movie was found question 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30368c72-6ea5-4630-8914-d0d13f1501c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Stream Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af4b3f27-a536-427c-a438-3023a8525438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Choose any five entities from the data set. These can be specific movies, actors, crews, etc, or more abstract concepts such as specific genres, etc. The main criteria is that the entities chosen must have a trackable wiki page. Set up a stream processing job that will track events for the chosen entities from the wikimedia Events Platform. These tracking jobs should provide some simple metrics. These metrics should be stored in a database or file (depending on the platform used). At least one of the metrics should be of the \"alert\" type (meaning some event that would require further action. For instance imagine wanting to be notified each time a specific user makes a change. Capture this \"alert\" and mimic an alerting system by routing these events to a different file/database.) These tables/data do not need to be shared, but the structure of the output should be clearly noted in the code and/or markdown cells. Additionally, a brief explanation/overview of this section should be provided in a separate markdown cell or in the project readme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f273c4ef-2457-4e56-b49e-fce5f918fe85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Définition des Entités et Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39aa6c82-75f6-4e06-9e99-3f17d349e70a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Path management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0957a6a1-e3bb-4787-a8e3-86c04b46e084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Where the python script will write raw JSONs (and Spark will read from)\n",
    "ingestion_volume = 'wiki_stream_buffer'\n",
    "RAW_INGESTION_PATH = f'/Volumes/{catalog}/{uc_schema_raw_events}/{ingestion_volume}/'\n",
    "\n",
    "# Where Spark will write the alerts\n",
    "alert_volume = 'wiki_alerts'\n",
    "ALERT_PATH = f'/Volumes/{catalog}/{uc_schema_raw_events}/{alert_volume}/critical_events/'\n",
    "\n",
    "# Where Spark will write the Metrics Table (Delta)\n",
    "METRICS_TABLE_PATH = f'{catalog}.{uc_schema_raw_events}.wiki_entity_metrics'\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint_volume = 'wiki_pipeline'\n",
    "CHECKPOINT_PATH = f'/Volumes/{catalog}/{db_schema_checkpoints}/{checkpoint_volume}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7234da55-27ea-4020-8c22-6f48b16446fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cleaning of raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96039881-cec3-4fd9-98a2-9f8e4cb621d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.rm(CHECKPOINT_PATH, True)\n",
    "dbutils.fs.rm(RAW_INGESTION_PATH, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8457667-b24b-421d-8351-b089f30fa4f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Objects\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{uc_schema_raw_events}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db_schema_checkpoints}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{uc_schema_raw_events}.{ingestion_volume}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{uc_schema_raw_events}.{alert_volume}\") \n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{db_schema_checkpoints}.{checkpoint_volume}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1c4ed87-8968-4984-9635-3f4fd80aa16d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure directories exist\n",
    "dbutils.fs.mkdirs(RAW_INGESTION_PATH)\n",
    "dbutils.fs.mkdirs(ALERT_PATH)\n",
    "dbutils.fs.mkdirs(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9deef9-a8ff-47e5-b0c6-1cd829dd5c83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Entities to tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf7fdd2-454e-40f7-bd7f-bfc439932bd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking entities: ['Dune', 'Tom Hanks', 'Comedy', 'Breaking Bad', 'France']\n"
     ]
    }
   ],
   "source": [
    "ENTITIES_TO_TRACK = [\n",
    "    \"Dune\",         # Film\n",
    "    \"Tom Hanks\",    # Actor\n",
    "    \"Comedy\",       # Genre\n",
    "    \"Breaking Bad\", # TV Series\n",
    "    \"France\"        # Country\n",
    "]\n",
    "print(f\"Tracking entities: {ENTITIES_TO_TRACK}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21239891-b061-49ec-88eb-b11413ae6bf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Schema definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5e2897-51b6-44ce-9129-b314f22676a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Meta schema (nested)\n",
    "meta_schema = StructType([\n",
    "    StructField(\"uri\", StringType(), True),\n",
    "    StructField(\"request_id\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"dt\", StringType(), True),\n",
    "    StructField(\"domain\", StringType(), True),\n",
    "    StructField(\"stream\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Length schema (nested)\n",
    "length_schema = StructType([\n",
    "    StructField(\"old\", IntegerType(), True),\n",
    "    StructField(\"new\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Revision schema (nested)\n",
    "revision_schema = StructType([\n",
    "    StructField(\"old\", LongType(), True),\n",
    "    StructField(\"new\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Main recent change schema\n",
    "recentchange_schema = StructType([\n",
    "    StructField(\"$schema\", StringType(), True),\n",
    "    StructField(\"meta\", meta_schema, True),\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"namespace\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"comment\", StringType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"bot\", BooleanType(), True),\n",
    "    StructField(\"minor\", BooleanType(), True),\n",
    "    StructField(\"patrolled\", BooleanType(), True),\n",
    "    StructField(\"length\", length_schema, True),\n",
    "    StructField(\"revision\", revision_schema, True),\n",
    "    StructField(\"server_url\", StringType(), True),\n",
    "    StructField(\"server_name\", StringType(), True),\n",
    "    StructField(\"wiki\", StringType(), True),\n",
    "    StructField(\"parsedcomment\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04a6d4ce-e6a3-44b0-aef7-a18a6ba45516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestion - Python Script to fetch Wiki Events and write to Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b15d05b-5cbf-464b-a01c-dfa27aa415a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ingestion to: /Volumes/imdb_db/raw_events/wiki_stream_buffer/\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "output_path = RAW_INGESTION_PATH \n",
    "print(f\"Starting ingestion to: {output_path}\")\n",
    "\n",
    "# Duration of the ingestion\n",
    "duration_minutes = 2\n",
    "stop_time = datetime.now() + timedelta(minutes=duration_minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d28c89-ce2d-4f95-985b-3ce18b2897c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Initilaize Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e28d12-3428-4dd7-8a87-a28142c67631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for events regarding: ['Dune', 'Tom Hanks', 'Comedy', 'Breaking Bad', 'France']\nWill stop automatically at: 2025-12-14 21:44:24.686783\n"
     ]
    }
   ],
   "source": [
    "stream = EventStreams(streams=['recentchange'])\n",
    "\n",
    "stream.register_filter(type='edit') \n",
    "\n",
    "print(f\"Listening for events regarding: {ENTITIES_TO_TRACK}\")\n",
    "print(f\"Will stop automatically at: {stop_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c376a84-a6e3-467a-8a01-63734c052f83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pre-Filtering\n",
    "To make the Spark job effective, we assume we only want to save events related to our specific list (or everything if you prefer).\n",
    "Since specific entities like \"Tom Hanks\" are edited rarely, \n",
    "We will save everything so the pipeline processes data, but mark the ones in our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74cc9d48-83f3-4891-b401-e53dad7d26bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 50 events...\nIngested 100 events...\nIngested 150 events...\nIngested 200 events...\nIngested 250 events...\nIngested 300 events...\nIngested 350 events...\nIngested 400 events...\nIngested 450 events...\nIngested 500 events...\nIngested 550 events...\nIngested 600 events...\nIngested 650 events...\nIngested 700 events...\nIngested 750 events...\nIngested 800 events...\nIngested 850 events...\nIngested 900 events...\nIngested 950 events...\nIngested 1000 events...\nIngested 1050 events...\nIngested 1100 events...\nIngested 1150 events...\nIngested 1200 events...\nIngested 1250 events...\nIngested 1300 events...\nIngested 1350 events...\nIngested 1400 events...\nIngested 1450 events...\nIngested 1500 events...\nIngested 1550 events...\nIngested 1600 events...\nIngested 1650 events...\nIngested 1700 events...\nIngested 1750 events...\nIngested 1800 events...\nIngested 1850 events...\nIngested 1900 events...\nIngested 1950 events...\nTime limit reached. Stopping ingestion.\nIngestion finished. Total files written: 1990\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    count = 0\n",
    "    # Loop to get streaming data\n",
    "    for change in stream:\n",
    "        if datetime.now() > stop_time:\n",
    "            print(\"Time limit reached. Stopping ingestion.\")\n",
    "            break\n",
    "            \n",
    "        # Get the title\n",
    "        title = change.get('title', '')\n",
    "\n",
    "        # Extract ID for filename\n",
    "        revision_id = change.get('revision', {}).get('new', 'unknown')\n",
    "        if revision_id == 'unknown':\n",
    "            revision_id = change.get('id', datetime.now().timestamp())\n",
    "            \n",
    "        file_name = f\"{output_path}/event_{revision_id}.json\"\n",
    "        \n",
    "        # Write event to file\n",
    "        with open(file_name, 'w') as f:\n",
    "            json.dump(change, f)\n",
    "            \n",
    "        count += 1\n",
    "        if count % 50 == 0:\n",
    "            print(f\"Ingested {count} events...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Stream stopped or error occurred: {e}\")\n",
    "\n",
    "print(f\"Ingestion finished. Total files written: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2d6c5f8-9686-489d-a906-12b32e2eb2fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Spark Structured Streaming Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de348dd-4bbe-4140-bc4f-bb9109e9087d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Read Stream from the Volume (instead of \"rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e67bb53-5b1c-42bb-a3b8-e6beaae23014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_stream_df = (\n",
    "    spark.readStream\n",
    "    .format(\"json\")\n",
    "    .schema(recentchange_schema)\n",
    "    .option(\"maxFilesPerTrigger\", 10)\n",
    "    .load(RAW_INGESTION_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941dafc6-b65d-46ff-9a5a-5355ceaf02fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Filter & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c06d17-b0d5-4804-9068-3f7abbae15d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "processed_stream_df = raw_stream_df.filter(\n",
    "    F.col(\"title\").isin(ENTITIES_TO_TRACK)\n",
    ").select(\n",
    "    F.col(\"title\").alias(\"entity\"),\n",
    "    F.col(\"timestamp\"),\n",
    "    F.col(\"user\"),\n",
    "    F.col(\"bot\"),\n",
    "    F.col(\"server_name\"),\n",
    "    F.col(\"comment\")\n",
    ").withColumn(\n",
    "    \"event_time\", F.col(\"timestamp\").cast(\"timestamp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf25ff1-1adb-4d92-a84b-2327cd805a0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define Alert Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c82bb8b4-c315-4e2f-a928-8e492e231d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Alert if: Entity is \"Dune\" AND it was edited by a Bot (mimicking the Target Code logic)\n",
    "alert_stream_df = processed_stream_df.filter(\n",
    "    (F.col(\"entity\") == \"Dune\") & \n",
    "    (F.col(\"bot\") == True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b36f9045-18a0-4975-8733-7911a52ac2e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define Metrics Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e09fbb0-db5b-4ae1-8ef6-30a3b01a6e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count events per entity every 10 minutes\n",
    "metrics_stream_df = processed_stream_df.withWatermark(\"event_time\", \"10 minutes\").groupBy(\n",
    "    F.window(F.col(\"event_time\"), \"10 minutes\"),\n",
    "    F.col(\"entity\")\n",
    ").count().withColumnRenamed(\"count\", \"event_count\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37a2650c-7d7c-4977-aa35-5ac10ba796ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Writing the streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbd9f2b2-f476-4b48-80c7-838e1adcbbdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Read Stream from volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7406172-e435-41a9-9876-99f7ca0d8a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_stream_df = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\") \n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .schema(recentchange_schema) \n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}autoload_schema\")\n",
    "    .load(RAW_INGESTION_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e487559-5f6c-4368-954d-eec8a03d42f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Filtering the target entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40179641-466d-4ba5-81f8-5fb2aa4d119e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning and projecting of the timestamps for the watermark\n",
    "processed_stream_df = raw_stream_df.select(\n",
    "    F.col(\"title\"),\n",
    "    F.col(\"bot\"),\n",
    "    F.col(\"user\"),\n",
    "    F.to_timestamp(F.col(\"meta.dt\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\").alias(\"event_time\") \n",
    ").filter(\n",
    "    F.col(\"title\").isin(ENTITIES_TO_TRACK) \n",
    ").withColumn(\n",
    "    \"entity\", F.col(\"title\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02e29391-5812-45a3-ab5e-4c7f1861ef41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Defining the alert and metric logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d02bc9d3-af0c-4389-ab26-4a345b27fcf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_alert = processed_stream_df.filter(\n",
    "    (F.col(\"entity\") == \"Dune\") & (F.col(\"bot\") == True)\n",
    ").select(\n",
    "    \"event_time\", \"entity\", \"user\", \"bot\", F.lit(\"CRITICAL: Bot edit on DUNE page\").alias(\"alert_message\")\n",
    ")\n",
    "\n",
    "metrics_stream_df = processed_stream_df.withWatermark(\"event_time\", \"10 minutes\").groupBy(\n",
    "    F.window(F.col(\"event_time\"), \"10 minutes\"),\n",
    "    F.col(\"entity\")\n",
    ").count().withColumnRenamed(\"count\", \"event_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "296b6a39-c59e-4faf-9303-250d0b04ca6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stream writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3195ba3-ac8a-4b2a-bcfe-69d2a276a560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage de la lecture des fichiers JSON depuis: /Volumes/imdb_db/raw_events/wiki_stream_buffer/\n\nDémarrage des requêtes de streaming en mode AvailableNow...\n"
     ]
    }
   ],
   "source": [
    "# Alert writing\n",
    "query_alerts: StreamingQuery = (\n",
    "    df_alert.writeStream\n",
    "    .format(\"json\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}alerts\")\n",
    "    .option(\"path\", ALERT_PATH)\n",
    "    .queryName(\"wiki_alerts_writer\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Metrics writing\n",
    "query_metrics: StreamingQuery = (\n",
    "    metrics_stream_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}metrics\")\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(METRICS_TABLE_PATH) \n",
    ")\n",
    "\n",
    "print(\"Streaming jobs started, the cell will wait the end of the micro-batch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "320f906a-ca82-4fa6-b25d-4c47215d74f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Synchronisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73669036-fedb-42dc-8ecd-4000da2e1eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_alerts.awaitTermination()\n",
    "print(\"Alert Stream done\")\n",
    "\n",
    "query_metrics.awaitTermination()\n",
    "print(\"Metrics Stream donc\")\n",
    "\n",
    "print(\"\\nmicro-batch is done. The result are ready for the verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bdadd0a-c335-4221-90eb-4fdfe2bd93f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Results verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf4ea55e-8a64-4574-82bb-8d91d78ac7c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Checking metrics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc28e7a7-2123-40f3-9cdf-2395f7882b2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.sql(f\"SELECT * FROM {METRICS_TABLE_PATH} ORDER BY window DESC\").show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"Error in the reading of the delta table: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25ee9591-1677-4a00-a9cb-b326bc57ef62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Checking alert system files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55c13549-7797-4d5a-b8d2-01ae5be5d4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Checking Metrics Table (Delta) ---\n+------+------+-----------+\n|window|entity|event_count|\n+------+------+-----------+\n+------+------+-----------+\n\n\n--- 2. Checking Alert System (Files) ---\nFound 1353 alert files.\nSample Alert Content:\n+----------+------+----+---+\n|event_time|entity|user|bot|\n+----------+------+----+---+\n+----------+------+----+---+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "files = dbutils.fs.ls(ALERT_PATH)\n",
    "print(f\"Found {len(files)} alert files.\")\n",
    "\n",
    "if len(files) > 0:\n",
    "    print(\"Sample Alert Content:\")\n",
    "    alert_read_schema = StructType([\n",
    "        StructField(\"event_time\", StringType(), True),\n",
    "        StructField(\"entity\", StringType(), True),\n",
    "        StructField(\"user\", StringType(), True),\n",
    "        StructField(\"bot\", BooleanType(), True)\n",
    "    ])\n",
    "    \n",
    "    (spark.read\n",
    "        .schema(alert_read_schema)\n",
    "        .json(ALERT_PATH)\n",
    "        .show(5, truncate=False)\n",
    "    )\n",
    "else:\n",
    "    print(\"No alerts generated yet (La condition 'Dune' + 'Bot' n'a pas été satisfaite dans ce lot de données).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad20517e-e41d-48d0-9487-eeacdcb671d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Clean the Volume of Raw Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b054cdc1-ea1c-44e7-9b02-31dbb960581f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run this code in case of error in the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e019002e-f9ab-4dcc-a208-5318bc07fd89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettoyage des fichiers bruts dans : /Volumes/imdb_db/raw_events/wiki_stream_buffer/\n✅ Nettoyage terminé. 1875 fichiers JSON supprimés du Volume d'ingestion.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Delete the json files that have been processed.\n",
    "#print(f\"Cleaning of raw files in : {RAW_INGESTION_PATH}\")\n",
    "\n",
    "#try:\n",
    "#    files_to_delete = dbutils.fs.ls(RAW_INGESTION_PATH)\n",
    "#    count = 0\n",
    "#    for file_info in files_to_delete:\n",
    "#        if file_info.name.endswith(\".json\"):\n",
    "#            dbutils.fs.rm(file_info.path)\n",
    "#            count += 1\n",
    "#    \n",
    "#    print(f\"Cleaning done {count} JSON files deleted\")\n",
    "#    \n",
    "#except Exception as e:\n",
    "#    print(f\"Error : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577d5632-5b7a-404d-9b42-fc2f35db36dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Check Results"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project_BigData",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}